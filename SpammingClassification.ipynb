{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections, functools, itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spam.csv\", encoding=\"latin-1\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "df.columns = [\"label\", \"text\", \"dummy1\", \"dummy2\", \"dummy3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4825, 'spam': 747})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "12\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for col in [\"dummy1\", \"dummy2\", \"dummy3\"]:\n",
    "    print(np.sum(~df[col].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"dummy1\", \"dummy2\", \"dummy3\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(df.shape[0]), test_size=0.2, random_state=42, shuffle=True, \n",
    "                                       stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 3859, 'spam': 598})"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(df.loc[train_idx].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 966, 'spam': 149})"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(df.loc[test_idx].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[train_idx]\n",
    "df_test = df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 1115)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelbinarizer = LabelBinarizer()\n",
    "Y = labelbinarizer.fit_transform(df[\"label\"])\n",
    "\n",
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]\n",
    "\n",
    "len(Y_train), len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(text):\n",
    "    text = text.lower() # lower case\n",
    "    text = re.sub('<[^<>]+>', ' ', text);  # contain '<' or '>')... replace with a space\n",
    "    text = re.sub('[0-9]+', 'number', text)  # mask for numbers\n",
    "    text = re.sub('(http|https)://[^\\s]*', 'httpaddr', text)  # mask for https header\n",
    "    text = re.sub('[$]+', 'dollar', text)  # nask for '$' sign\n",
    "    return text\n",
    "\n",
    "def text2token(raw_text):\n",
    "    text = preProcess(raw_text)\n",
    "    text = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]', text)\n",
    "    tokens = []\n",
    "    for subtext in text:\n",
    "        # Remove any non alphanumeric characters\n",
    "        subtext = re.sub('[^a-zA-Z0-9]', '', subtext);\n",
    "        if not len(subtext): \n",
    "            continue\n",
    "        tokens += word_tokenize(subtext)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"clean_text\"] = df_train[\"text\"].apply(text2token)\n",
    "df_test[\"clean_text\"] = df_test[\"text\"].apply(text2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184                      [going, on, nothing, great, bye]\n",
       "2171                [i, wont, so, wat, s, wit, the, guys]\n",
       "5422    [ok, k, sry, i, knw, number, siva, tats, y, i,...\n",
       "4113    [where, are, you, what, do, you, do, how, can,...\n",
       "4588    [have, you, not, finished, work, yet, or, some...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"clean_text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count-based SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(itertools.chain(*df_train[\"clean_text\"]))\n",
    "vocab_dict = collections.Counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 5\n",
    "\n",
    "len([v for _,v in vocab_dict.items() if v >= threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1552, 1552)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "cnt = 0\n",
    "for w, v in vocab_dict.items():\n",
    "    if v >= threshold: # record the word which appears more than threshold times\n",
    "        word2idx[w] = cnt\n",
    "        idx2word[w] = w\n",
    "        cnt +=1\n",
    "\n",
    "len(word2idx), len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfeature(wordlist):\n",
    "    feature = np.zeros(len(word2idx))\n",
    "    for w in wordlist:\n",
    "        if w not in word2idx:\n",
    "            continue\n",
    "        if feature[word2idx[w]] == 0:\n",
    "            feature[word2idx[w]] = 1\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(df_train[\"clean_text\"].apply(getfeature).values.tolist())\n",
    "X_test = np.array(df_test[\"clean_text\"].apply(getfeature).values.tolist())\n",
    "# save data frame for random forest model\n",
    "X_train_RF = X_train\n",
    "X_test_RF = X_test\n",
    "Y_train_RF = Y_train\n",
    "Y_test_RF = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "\n",
    "SVM = svm.SVC(C=0.1, kernel='rbf')\n",
    "SVM.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 94.73%\n",
      "Testing accuracy = 94.08%\n"
     ]
    }
   ],
   "source": [
    "train_predictions = SVM.predict(X_train)\n",
    "train_acc = np.sum(train_predictions == Y_train.ravel())/Y_train.shape[0]\n",
    "print(f'Training accuracy = {100*train_acc:.2f}%')\n",
    "\n",
    "test_predictions = SVM.predict(X_test)\n",
    "test_acc = np.sum(test_predictions == Y_test.ravel())/Y_test.shape[0]\n",
    "print(f'Testing accuracy = {100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7155172413793103\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(Y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       966\n",
      "           1       1.00      0.56      0.72       149\n",
      "\n",
      "    accuracy                           0.94      1115\n",
      "   macro avg       0.97      0.78      0.84      1115\n",
      "weighted avg       0.94      0.94      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "dimension = 100\n",
    "model = Word2Vec(sentences=itertools.chain(df_train[\"clean_text\"]), \n",
    "                 size=dimension, window=5, min_count=1, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4457.000000\n",
       "mean       16.054745\n",
       "std        11.725931\n",
       "min         0.000000\n",
       "5%          4.000000\n",
       "50%        12.000000\n",
       "95%        34.000000\n",
       "max       190.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"text_len\"] = df_train[\"clean_text\"].map(len)\n",
    "df_train[\"text_len\"].describe(percentiles=[0.05,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getembedding(wordlist, maxlen=35):\n",
    "    normalized_wordlist = wordlist[:maxlen]\n",
    "    if len(normalized_wordlist) < maxlen: # pad to maxlen\n",
    "        normalized_wordlist += [\" \"] * (maxlen-len(normalized_wordlist))\n",
    "    return np.array([model.wv[w] if w in model.wv else np.zeros(dimension) for w in normalized_wordlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4457, 35, 100), (1115, 35, 100))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(df_train[\"clean_text\"].apply(getembedding).values.tolist())\n",
    "X_test = np.array(df_test[\"clean_text\"].apply(getembedding).values.tolist())\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count-based Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Testing) Random Forest Score : 0.9811659192825112\n",
      "Testing Accuarcy: 98.116592％ (sklearn.ensemble._forest)\n",
      "Training Accuarcy RF: 99.977563％\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "Random_forest = RandomForestClassifier(n_estimators=50)\n",
    "Random_forest.fit(X_train_RF, Y_train_RF.ravel())\n",
    "\n",
    "randomForest_predict = Random_forest.predict(X_test_RF)\n",
    "randomForest_score = metrics.accuracy_score(Y_test_RF.ravel(), randomForest_predict)\n",
    "print(\"(Testing) Random Forest Score :\", randomForest_score)\n",
    "\n",
    "Y_hat = Random_forest.predict(X_test_RF)\n",
    "n = np.size(Y_test_RF)\n",
    "print('Testing Accuarcy: {:.6f}％ ({})'.format(sum(np.int_(Y_hat==Y_test_RF.ravel()))*100./n, Random_forest.__module__))\n",
    "\n",
    "n=np.size(Y_train_RF.ravel())\n",
    "Y_hat_RF = Random_forest.predict(X_train_RF)\n",
    "print('Training Accuarcy RF: {:.6f}％'.format(sum(np.int_(Y_hat_RF==Y_train_RF.ravel()))*100./n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "stopwords= nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_to_word(sentence):\n",
    "    s = \"\".join(x for x in sentence if x not in string.punctuation)\n",
    "    temp = s\n",
    "    temp = s.lower().split(' ')\n",
    "    temp2 = [x for x in temp if x not in stopwords]\n",
    "    return temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(analyzer=clean_to_word)\n",
    "vector_output = vect.fit_transform(df['text'])\n",
    "# print(vect.get_feature_names()[0:3])\n",
    "# print (vector_output [0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9339</th>\n",
       "      <th>9340</th>\n",
       "      <th>9341</th>\n",
       "      <th>9342</th>\n",
       "      <th>9343</th>\n",
       "      <th>9344</th>\n",
       "      <th>9345</th>\n",
       "      <th>9346</th>\n",
       "      <th>9347</th>\n",
       "      <th>9348</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.184905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.147150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 9349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "0     0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1     0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2     0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3     0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4     0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "5567  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "5568  0.184905   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "5569  0.147150   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "5570  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "5571  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "      9339  9340  9341  9342  9343  9344  9345  9346  9347  9348  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5567   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5568   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5569   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5570   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5571   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5572 rows x 9349 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vector_output.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering, can explore further later\n",
    "import re\n",
    "df['len'] = df['text'].apply(lambda x : len(x) - x.count(\" \"))\n",
    "df['long_number'] = df['text'].apply(lambda x : len(re.findall('\\d{7,}',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def count_punct (text):\n",
    "    count = sum([1 for x in text if x in string.punctuation])\n",
    "    pp = round(100*count/(len(text)-text.count(\" \")),3)\n",
    "    return pp\n",
    "\n",
    "df['punct'] = df['text'].apply(lambda x : count_punct(x))\n",
    "\n",
    "testlink = \"hello buddwwy http how com are you.co ww ww.\"\n",
    "\n",
    "def  website (text):\n",
    "    if (len(re.findall('www|http|com|\\.co',text))>0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['website'] = df['text'].apply(lambda x : website(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = pd.concat([df['len'],df['long_number'],df['punct'],df['website'],pd.DataFrame(vector_output.toarray())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0663983964291311, 'long_number'),\n",
       " (0.03513170729545177, 'len'),\n",
       " (0.028647347861842486, 8514),\n",
       " (0.027537843805422076, 2208),\n",
       " (0.025376108828532428, 1932),\n",
       " (0.01944541435765632, 3555),\n",
       " (0.012696884982000707, 5471),\n",
       " (0.012604581945657198, 7805),\n",
       " (0.012492501311375826, 6540),\n",
       " (0.012094708202485586, 6880)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features,df['label'])\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=None,n_jobs=-1)\n",
    "rf_model = rf.fit(x_train,y_train)\n",
    "sorted(zip(rf_model.feature_importances_,x_train.columns),reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 1.0 / Recall : 0.842391 / fscore : 0.914454 / Accuracy: 0.979182\n"
     ]
    }
   ],
   "source": [
    "y_pred=rf_model.predict(x_test)\n",
    "precision,recall,fscore,support =score(y_test,y_pred,pos_label='spam', average ='binary')\n",
    "print('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,6),round(recall,6),round(fscore,6),round((y_pred==y_test).sum()/len(y_test),6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from utility_torch import Timer, MyDataset\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(BaseModel,self).__init__(**kwargs)\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(in_features=100*35,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128,out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32,out_features=1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # concat + fully connected\n",
    "        X = self.Flatten(X)\n",
    "        output = self.FC(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                 [-1, 3500]               0\n",
      "            Linear-2                  [-1, 128]         448,128\n",
      "              ReLU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 32]           4,128\n",
      "              ReLU-5                   [-1, 32]               0\n",
      "            Linear-6                    [-1, 1]              33\n",
      "           Sigmoid-7                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 452,289\n",
      "Trainable params: 452,289\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 1.73\n",
      "Estimated Total Size (MB): 1.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Net = BaseModel()\n",
    "summary(Net,input_size=(1,35,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dBlock(nn.Module):\n",
    "    \"\"\"1-dimension convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, **kwargs):\n",
    "        super(Conv1dBlock, self).__init__(**kwargs)\n",
    "        self.conv1d = nn.Conv1d(in_channels,out_channels,kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # batch_size seq_len embedding_size -> batch_size embedding_size seq_len\n",
    "        X = X.transpose(2,1)\n",
    "        X = self.conv1d(X)\n",
    "        X = self.relu(X)\n",
    "        output = torch.squeeze(self.pool(X),axis=-1) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 16, 34]           3,216\n",
      "              ReLU-2               [-1, 16, 34]               0\n",
      " AdaptiveMaxPool1d-3                [-1, 16, 1]               0\n",
      "================================================================\n",
      "Total params: 3,216\n",
      "Trainable params: 3,216\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Net = Conv1dBlock(in_channels=100, out_channels=16, kernel_size=2, stride=1, padding=\"valid\")\n",
    "summary(Net, input_size=(35,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"TEXTCNN\"\"\"\n",
    "    def __init__(self, embedding_dim, num_channels, kernel_sizes, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.convs = nn.Sequential()\n",
    "        for idx, (c, k) in enumerate(zip(num_channels, kernel_sizes)):\n",
    "            self.convs.add_module(f'conv{idx+1}', \n",
    "                                  Conv1dBlock(in_channels=embedding_dim, out_channels=c, kernel_size=k,\n",
    "                                              stride=1, padding=\"valid\")\n",
    "                                )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(in_features=c*len(kernel_sizes),out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32,out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16,out_features=1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, X):\n",
    "        encoding = torch.cat([conv(X) for conv in self.convs],\n",
    "                             axis=1)\n",
    "        output = self.dropout(self.FC(encoding))\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 16, 34]           3,216\n",
      "              ReLU-2               [-1, 16, 34]               0\n",
      " AdaptiveMaxPool1d-3                [-1, 16, 1]               0\n",
      "       Conv1dBlock-4                   [-1, 16]               0\n",
      "            Conv1d-5               [-1, 16, 31]           8,016\n",
      "              ReLU-6               [-1, 16, 31]               0\n",
      " AdaptiveMaxPool1d-7                [-1, 16, 1]               0\n",
      "       Conv1dBlock-8                   [-1, 16]               0\n",
      "            Conv1d-9               [-1, 16, 26]          16,016\n",
      "             ReLU-10               [-1, 16, 26]               0\n",
      "AdaptiveMaxPool1d-11                [-1, 16, 1]               0\n",
      "      Conv1dBlock-12                   [-1, 16]               0\n",
      "           Conv1d-13               [-1, 16, 16]          32,016\n",
      "             ReLU-14               [-1, 16, 16]               0\n",
      "AdaptiveMaxPool1d-15                [-1, 16, 1]               0\n",
      "      Conv1dBlock-16                   [-1, 16]               0\n",
      "           Linear-17                   [-1, 32]           2,080\n",
      "             ReLU-18                   [-1, 32]               0\n",
      "           Linear-19                   [-1, 16]             528\n",
      "             ReLU-20                   [-1, 16]               0\n",
      "           Linear-21                    [-1, 1]              17\n",
      "          Dropout-22                    [-1, 1]               0\n",
      "          Sigmoid-23                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 61,889\n",
      "Trainable params: 61,889\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Net = TextCNN(embedding_dim=100, num_channels=[16, 16, 16, 16], kernel_sizes=[2, 5, 10, 20])\n",
    "summary(Net, input_size=(35,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self,embedding_dim, hidden_size, num_layers, bi, **kwargs):\n",
    "        super(LSTMBlock, self).__init__(**kwargs)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,bidirectional=bi)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # output: batch_size,seq_len,hidden_size*num_directions\n",
    "        # h_n, c_n: num_layers*num_directions,batch_size,hidden_size\n",
    "        output, (h_n, c_n) = self.lstm(X)  \n",
    "        #print(output.shape,h_n.shape,c_n.shape)\n",
    "        #torch.cat([output[:,0,:],output[:,-1,:]],axis=1)\n",
    "        return h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 16])\n"
     ]
    }
   ],
   "source": [
    "Net = LSTMBlock(embedding_dim=100, hidden_size=16, num_layers=2, bi=False)\n",
    "_input = torch.randn(size=(32,35,100))\n",
    "_output = Net(_input)\n",
    "print(_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FC(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, bi=False, **kwargs):\n",
    "        super(LSTM_FC,self).__init__(**kwargs)\n",
    "        self.lstmblock = LSTMBlock(embedding_dim=embedding_dim, \n",
    "                                   hidden_size=hidden_size, \n",
    "                                   num_layers=num_layers, bi=bi)\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(in_features=128,out_features=32),\n",
    "            nn.Linear(in_features=32,out_features=1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.lstmblock(X)\n",
    "        # num_layers*num_directions,batch_size,hidden_size --> \n",
    "        # batch_size, num_layers*num_directions, hidden_size\n",
    "        X = X.transpose(1,0)  \n",
    "        X = self.Flatten(X)\n",
    "        output = self.FC(X)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "Net = LSTM_FC(100, 64, 2)\n",
    "\n",
    "_input = torch.randn(size=(32,35,100))\n",
    "_output = Net(_input)\n",
    "print(_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanila_acc(y_hat, y, threshold):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    y_pred = torch.zeros(size=y_hat.shape,dtype=y_hat.dtype)\n",
    "    y_pred[y_hat > threshold] = 1\n",
    "    cmp = y_pred == y\n",
    "    return float(torch.sum(cmp))/y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "y_hat = torch.tensor([[0.57],[0.38],[0.49],[0.55],[0.40]])\n",
    "y = torch.tensor([[1],[0],[0],[0],[0]])\n",
    "vanila_acc(y_hat,y,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, data_iter, loss, acc_func, threshold):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset\"\"\"\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    metric = [0]*4 # loss acc recall, batch_num\n",
    "    with torch.no_grad(): # stop updating the model\n",
    "        for i, (X, y) in enumerate(data_iter):\n",
    "            y_hat = net(X.type(torch.float32))\n",
    "            metric[0] += loss(y_hat,y.type(torch.float32))\n",
    "            metric[1] += acc_func(y_hat, y.type(torch.float32),threshold=threshold)\n",
    "            metric[2] += torch.sum(torch.mul(y_hat, y.type(torch.float32))) / torch.sum(y_hat)\n",
    "            metric[3] += 1\n",
    "    return metric[0]/metric[3], metric[1]/metric[3], metric[2]/metric[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, train_iter, val_iter, num_epochs, lr, patience, savepath):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv1d or type(m) == nn.Conv2d:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    net.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    loss = nn.BCELoss()\n",
    "    since_last_best, best_val = 0, (float('-inf'),0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        timer = Timer() # record time each epoch\n",
    "        metric = [0]*5 # loss acc recall batch_num\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print(f'{i+1}/{len(train_iter)}'+'\\r',end='')\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X.type(torch.float32))\n",
    "            l = loss(y_hat, y.type(torch.float32))\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            # evaluate on train_iter\n",
    "            with torch.no_grad():\n",
    "                metric[0] += l\n",
    "                metric[1] += vanila_acc(y_hat,y,threshold=0.5)\n",
    "                metric[2] += torch.sum(torch.mul(y_hat, y.type(torch.float32))) / torch.sum(y_hat)\n",
    "                metric[3] += 1  # tally batch num\n",
    "                metric[4] += X.shape[0]  # tally observation\n",
    "            timer.stop()\n",
    "        \n",
    "        # evaluate on val_iter\n",
    "        val_l, val_acc, val_recall = evaluate(net, val_iter, loss=loss, acc_func=vanila_acc, threshold=0.5)\n",
    "        val_f1 = 2 / (1/val_acc + 1/val_recall)\n",
    "        \n",
    "        # early stopping\n",
    "        if val_f1 > best_val[0]: \n",
    "            best_val = (val_f1, epoch+1)\n",
    "            since_last_best = 0\n",
    "            torch.save(net.state_dict(), savepath) # Save model\n",
    "        else:\n",
    "            since_last_best +=1\n",
    "            if since_last_best > patience:\n",
    "                print(f'early stopping! best_val:{best_val[0]:.3f} epoch{best_val[1]}')\n",
    "                break\n",
    "        \n",
    "        # on_epoch_end\n",
    "        print(f'epoch{epoch+1} --> ',\n",
    "              f'{metric[4] / timer.sum():.1f} examples/sec',\n",
    "              f'train_loss {metric[0]/metric[3]:.3f} train acc {metric[1]/metric[3]:.3f} train recall {metric[2]/metric[3]:.3f}',\n",
    "              f'val_loss {val_l:.3f} val acc {val_acc:.3f} val recall {val_recall:.3f} ')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_epochs, lr, seed, patience = 32, 100, 0.001, 42, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 35, 100]) torch.Size([32, 35, 100]) torch.Size([32, 35, 100])\n",
      "torch.Size([32, 1]) torch.Size([32, 1]) torch.Size([32, 1])\n",
      "<class 'torch.Tensor'>\n",
      "Counter({0: 28, 1: 4})\n"
     ]
    }
   ],
   "source": [
    "train_idx_NN, val_idx_NN = train_test_split(np.arange(X_train.shape[0]), test_size=0.2, random_state=42, \n",
    "                                            shuffle=True, stratify=Y_train)\n",
    "\n",
    "train_iter = data.DataLoader(MyDataset(X_train[train_idx_NN],Y_train[train_idx_NN]),\n",
    "                             batch_size, shuffle=True,num_workers=0)\n",
    "val_iter = data.DataLoader(MyDataset(X_train[val_idx_NN],Y_train[val_idx_NN]), \n",
    "                           batch_size, shuffle=False,num_workers=0)\n",
    "test_iter = data.DataLoader(MyDataset(X_test,Y_test), batch_size, shuffle=False,num_workers=0)\n",
    "\n",
    "for train, val, test in zip(train_iter, val_iter, test_iter):\n",
    "    print(train[0].shape,val[0].shape,test[0].shape)\n",
    "    print(train[1].shape,val[1].shape, test[1].shape)\n",
    "    print(type(train[0]))\n",
    "    print(collections.Counter(train[1].numpy().ravel()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"FC.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 -->  4429.2 examples/sec train_loss 0.447 train acc 0.865 train recall 0.159 val_loss 0.315 val acc 0.866 val recall 0.233 \n",
      "epoch2 -->  4660.1 examples/sec train_loss 0.222 train acc 0.907 train recall 0.407 val_loss 0.182 val acc 0.941 val recall 0.575 \n",
      "epoch3 -->  4594.3 examples/sec train_loss 0.108 train acc 0.970 train recall 0.696 val_loss 0.136 val acc 0.959 val recall 0.739 \n",
      "epoch4 -->  4753.4 examples/sec train_loss 0.062 train acc 0.984 train recall 0.825 val_loss 0.131 val acc 0.963 val recall 0.774 \n",
      "epoch5 -->  4684.5 examples/sec train_loss 0.040 train acc 0.990 train recall 0.876 val_loss 0.132 val acc 0.962 val recall 0.795 \n",
      "epoch6 -->  4594.5 examples/sec train_loss 0.027 train acc 0.993 train recall 0.910 val_loss 0.134 val acc 0.963 val recall 0.793 \n",
      "epoch7 -->  3954.3 examples/sec train_loss 0.019 train acc 0.995 train recall 0.932 val_loss 0.138 val acc 0.963 val recall 0.806 \n",
      "epoch8 -->  3847.8 examples/sec train_loss 0.011 train acc 0.998 train recall 0.942 val_loss 0.149 val acc 0.961 val recall 0.845 \n",
      "epoch9 -->  3902.6 examples/sec train_loss 0.007 train acc 0.999 train recall 0.960 val_loss 0.155 val acc 0.962 val recall 0.844 \n",
      "epoch10 -->  3950.0 examples/sec train_loss 0.003 train acc 1.000 train recall 0.986 val_loss 0.156 val acc 0.961 val recall 0.811 \n",
      "epoch11 -->  3549.7 examples/sec train_loss 0.002 train acc 1.000 train recall 0.990 val_loss 0.167 val acc 0.964 val recall 0.837 \n",
      "epoch12 -->  2679.6 examples/sec train_loss 0.002 train acc 1.000 train recall 0.993 val_loss 0.179 val acc 0.963 val recall 0.856 \n",
      "epoch13 -->  2671.6 examples/sec train_loss 0.001 train acc 1.000 train recall 0.995 val_loss 0.191 val acc 0.962 val recall 0.870 \n",
      "epoch14 -->  2675.7 examples/sec train_loss 0.001 train acc 1.000 train recall 0.997 val_loss 0.193 val acc 0.962 val recall 0.862 \n",
      "epoch15 -->  2628.4 examples/sec train_loss 0.001 train acc 1.000 train recall 0.971 val_loss 0.196 val acc 0.964 val recall 0.858 \n",
      "epoch16 -->  2704.0 examples/sec train_loss 0.001 train acc 1.000 train recall 0.980 val_loss 0.199 val acc 0.963 val recall 0.853 \n",
      "epoch17 -->  2737.1 examples/sec train_loss 0.000 train acc 1.000 train recall 0.998 val_loss 0.209 val acc 0.963 val recall 0.867 \n",
      "epoch18 -->  2632.3 examples/sec train_loss 0.000 train acc 1.000 train recall 0.981 val_loss 0.216 val acc 0.962 val recall 0.870 \n",
      "epoch19 -->  2657.8 examples/sec train_loss 0.000 train acc 1.000 train recall 0.972 val_loss 0.222 val acc 0.961 val recall 0.874 \n",
      "epoch20 -->  2741.3 examples/sec train_loss 0.000 train acc 1.000 train recall 0.999 val_loss 0.223 val acc 0.963 val recall 0.869 \n",
      "epoch21 -->  2722.4 examples/sec train_loss 0.000 train acc 1.000 train recall 0.955 val_loss 0.228 val acc 0.963 val recall 0.871 \n",
      "epoch22 -->  2558.7 examples/sec train_loss 0.000 train acc 1.000 train recall 0.973 val_loss 0.232 val acc 0.963 val recall 0.871 \n",
      "epoch23 -->  2582.8 examples/sec train_loss 0.000 train acc 1.000 train recall 0.982 val_loss 0.233 val acc 0.964 val recall 0.868 \n",
      "epoch24 -->  1138.4 examples/sec train_loss 0.000 train acc 1.000 train recall 0.982 val_loss 0.242 val acc 0.962 val recall 0.875 \n",
      "epoch25 -->  1313.2 examples/sec train_loss 0.000 train acc 1.000 train recall 0.991 val_loss 0.240 val acc 0.964 val recall 0.868 \n",
      "epoch26 -->  2407.1 examples/sec train_loss 0.000 train acc 1.000 train recall 0.991 val_loss 0.248 val acc 0.963 val recall 0.873 \n",
      "epoch27 -->  2171.7 examples/sec train_loss 0.000 train acc 1.000 train recall 0.991 val_loss 0.251 val acc 0.963 val recall 0.873 \n",
      "epoch28 -->  2255.3 examples/sec train_loss 0.000 train acc 1.000 train recall 0.991 val_loss 0.253 val acc 0.964 val recall 0.872 \n",
      "epoch29 -->  2571.6 examples/sec train_loss 0.000 train acc 1.000 train recall 0.991 val_loss 0.256 val acc 0.964 val recall 0.872 \n",
      "early stopping! best_val:0.916 epoch24\n"
     ]
    }
   ],
   "source": [
    "Net = BaseModel()\n",
    "trained_net = train_model(Net, train_iter, val_iter, num_epochs, lr, patience,\n",
    "                          savepath=savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"textcnn.pth\"\n",
    "embedding_dim, num_channels, kernel_sizes = 100, [16,16,16,16], [2, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 -->  2533.4 examples/sec train_loss 0.554 train acc 0.867 train recall 0.136 val_loss 0.443 val acc 0.866 val recall 0.141 \n",
      "epoch2 -->  2529.8 examples/sec train_loss 0.500 train acc 0.866 train recall 0.156 val_loss 0.337 val acc 0.886 val recall 0.227 \n",
      "epoch3 -->  2569.8 examples/sec train_loss 0.429 train acc 0.907 train recall 0.227 val_loss 0.206 val acc 0.951 val recall 0.417 \n",
      "epoch4 -->  2790.5 examples/sec train_loss 0.398 train acc 0.919 train recall 0.262 val_loss 0.174 val acc 0.960 val recall 0.493 \n",
      "epoch5 -->  2614.9 examples/sec train_loss 0.390 train acc 0.922 train recall 0.273 val_loss 0.180 val acc 0.959 val recall 0.473 \n",
      "epoch6 -->  2724.5 examples/sec train_loss 0.397 train acc 0.921 train recall 0.269 val_loss 0.149 val acc 0.963 val recall 0.569 \n",
      "epoch7 -->  1690.9 examples/sec train_loss 0.380 train acc 0.923 train recall 0.282 val_loss 0.162 val acc 0.963 val recall 0.511 \n",
      "epoch8 -->  1610.2 examples/sec train_loss 0.375 train acc 0.925 train recall 0.287 val_loss 0.148 val acc 0.963 val recall 0.548 \n",
      "epoch9 -->  1834.1 examples/sec train_loss 0.375 train acc 0.926 train recall 0.289 val_loss 0.130 val acc 0.967 val recall 0.622 \n",
      "epoch10 -->  1642.8 examples/sec train_loss 0.364 train acc 0.927 train recall 0.291 val_loss 0.128 val acc 0.966 val recall 0.615 \n",
      "epoch11 -->  1774.9 examples/sec train_loss 0.369 train acc 0.926 train recall 0.295 val_loss 0.138 val acc 0.970 val recall 0.572 \n",
      "epoch12 -->  1831.3 examples/sec train_loss 0.358 train acc 0.931 train recall 0.297 val_loss 0.131 val acc 0.970 val recall 0.592 \n",
      "epoch13 -->  1793.3 examples/sec train_loss 0.349 train acc 0.928 train recall 0.307 val_loss 0.112 val acc 0.971 val recall 0.677 \n",
      "epoch14 -->  1547.4 examples/sec train_loss 0.349 train acc 0.936 train recall 0.312 val_loss 0.107 val acc 0.971 val recall 0.696 \n",
      "epoch15 -->  1593.0 examples/sec train_loss 0.349 train acc 0.930 train recall 0.307 val_loss 0.104 val acc 0.970 val recall 0.798 \n",
      "epoch16 -->  1674.3 examples/sec train_loss 0.362 train acc 0.929 train recall 0.298 val_loss 0.101 val acc 0.973 val recall 0.698 \n",
      "epoch17 -->  1745.4 examples/sec train_loss 0.340 train acc 0.937 train recall 0.314 val_loss 0.099 val acc 0.972 val recall 0.710 \n",
      "epoch18 -->  1690.9 examples/sec train_loss 0.349 train acc 0.934 train recall 0.309 val_loss 0.092 val acc 0.973 val recall 0.764 \n",
      "epoch19 -->  1638.2 examples/sec train_loss 0.353 train acc 0.937 train recall 0.307 val_loss 0.090 val acc 0.974 val recall 0.805 \n",
      "epoch20 -->  1791.8 examples/sec train_loss 0.348 train acc 0.931 train recall 0.306 val_loss 0.089 val acc 0.972 val recall 0.788 \n",
      "epoch21 -->  1495.0 examples/sec train_loss 0.346 train acc 0.928 train recall 0.307 val_loss 0.091 val acc 0.972 val recall 0.748 \n",
      "epoch22 -->  1648.8 examples/sec train_loss 0.341 train acc 0.930 train recall 0.311 val_loss 0.087 val acc 0.972 val recall 0.824 \n",
      "epoch23 -->  1752.3 examples/sec train_loss 0.344 train acc 0.934 train recall 0.302 val_loss 0.086 val acc 0.972 val recall 0.818 \n",
      "epoch24 -->  1565.8 examples/sec train_loss 0.353 train acc 0.933 train recall 0.311 val_loss 0.086 val acc 0.972 val recall 0.809 \n",
      "epoch25 -->  1728.6 examples/sec train_loss 0.361 train acc 0.924 train recall 0.298 val_loss 0.087 val acc 0.972 val recall 0.847 \n",
      "epoch26 -->  1420.7 examples/sec train_loss 0.342 train acc 0.934 train recall 0.313 val_loss 0.087 val acc 0.972 val recall 0.859 \n",
      "epoch27 -->  1521.8 examples/sec train_loss 0.333 train acc 0.935 train recall 0.327 val_loss 0.087 val acc 0.972 val recall 0.855 \n",
      "epoch28 -->  1602.3 examples/sec train_loss 0.353 train acc 0.927 train recall 0.303 val_loss 0.086 val acc 0.973 val recall 0.853 \n",
      "epoch29 -->  1643.5 examples/sec train_loss 0.340 train acc 0.935 train recall 0.319 val_loss 0.086 val acc 0.972 val recall 0.848 \n",
      "epoch30 -->  1747.1 examples/sec train_loss 0.353 train acc 0.931 train recall 0.298 val_loss 0.088 val acc 0.972 val recall 0.867 \n",
      "epoch31 -->  1690.9 examples/sec train_loss 0.349 train acc 0.935 train recall 0.310 val_loss 0.087 val acc 0.972 val recall 0.856 \n",
      "epoch32 -->  1576.8 examples/sec train_loss 0.344 train acc 0.935 train recall 0.314 val_loss 0.088 val acc 0.972 val recall 0.872 \n",
      "epoch33 -->  1702.2 examples/sec train_loss 0.338 train acc 0.938 train recall 0.319 val_loss 0.088 val acc 0.973 val recall 0.872 \n",
      "epoch34 -->  1581.0 examples/sec train_loss 0.350 train acc 0.939 train recall 0.312 val_loss 0.090 val acc 0.972 val recall 0.885 \n",
      "epoch35 -->  1677.4 examples/sec train_loss 0.334 train acc 0.934 train recall 0.320 val_loss 0.089 val acc 0.973 val recall 0.876 \n",
      "epoch36 -->  1532.9 examples/sec train_loss 0.345 train acc 0.933 train recall 0.312 val_loss 0.089 val acc 0.972 val recall 0.882 \n",
      "epoch37 -->  1496.9 examples/sec train_loss 0.346 train acc 0.932 train recall 0.310 val_loss 0.090 val acc 0.972 val recall 0.888 \n",
      "epoch38 -->  1631.5 examples/sec train_loss 0.344 train acc 0.931 train recall 0.308 val_loss 0.088 val acc 0.973 val recall 0.863 \n",
      "epoch39 -->  1690.1 examples/sec train_loss 0.352 train acc 0.931 train recall 0.302 val_loss 0.090 val acc 0.973 val recall 0.885 \n",
      "epoch40 -->  1677.4 examples/sec train_loss 0.340 train acc 0.938 train recall 0.317 val_loss 0.090 val acc 0.972 val recall 0.880 \n",
      "epoch41 -->  1530.9 examples/sec train_loss 0.344 train acc 0.938 train recall 0.313 val_loss 0.090 val acc 0.973 val recall 0.880 \n",
      "epoch42 -->  1377.5 examples/sec train_loss 0.341 train acc 0.934 train recall 0.314 val_loss 0.091 val acc 0.972 val recall 0.884 \n",
      "epoch43 -->  1598.7 examples/sec train_loss 0.350 train acc 0.926 train recall 0.303 val_loss 0.092 val acc 0.973 val recall 0.891 \n",
      "epoch44 -->  1457.9 examples/sec train_loss 0.349 train acc 0.931 train recall 0.309 val_loss 0.094 val acc 0.972 val recall 0.902 \n",
      "epoch45 -->  1669.6 examples/sec train_loss 0.346 train acc 0.933 train recall 0.311 val_loss 0.093 val acc 0.973 val recall 0.896 \n",
      "epoch46 -->  1575.4 examples/sec train_loss 0.347 train acc 0.932 train recall 0.304 val_loss 0.093 val acc 0.973 val recall 0.898 \n",
      "epoch47 -->  1599.4 examples/sec train_loss 0.350 train acc 0.933 train recall 0.304 val_loss 0.093 val acc 0.973 val recall 0.898 \n",
      "epoch48 -->  1725.2 examples/sec train_loss 0.346 train acc 0.934 train recall 0.310 val_loss 0.093 val acc 0.972 val recall 0.892 \n",
      "epoch49 -->  1686.1 examples/sec train_loss 0.346 train acc 0.938 train recall 0.316 val_loss 0.095 val acc 0.973 val recall 0.903 \n",
      "epoch50 -->  1698.2 examples/sec train_loss 0.348 train acc 0.932 train recall 0.306 val_loss 0.095 val acc 0.973 val recall 0.903 \n",
      "epoch51 -->  1532.8 examples/sec train_loss 0.355 train acc 0.939 train recall 0.308 val_loss 0.096 val acc 0.972 val recall 0.907 \n",
      "epoch52 -->  1614.6 examples/sec train_loss 0.341 train acc 0.937 train recall 0.315 val_loss 0.096 val acc 0.973 val recall 0.904 \n",
      "epoch53 -->  1572.0 examples/sec train_loss 0.340 train acc 0.939 train recall 0.314 val_loss 0.097 val acc 0.972 val recall 0.910 \n",
      "epoch54 -->  1834.1 examples/sec train_loss 0.345 train acc 0.931 train recall 0.310 val_loss 0.096 val acc 0.973 val recall 0.903 \n",
      "epoch55 -->  1665.7 examples/sec train_loss 0.339 train acc 0.934 train recall 0.315 val_loss 0.097 val acc 0.973 val recall 0.907 \n",
      "epoch56 -->  1694.2 examples/sec train_loss 0.351 train acc 0.930 train recall 0.306 val_loss 0.097 val acc 0.973 val recall 0.905 \n",
      "epoch57 -->  1608.0 examples/sec train_loss 0.348 train acc 0.932 train recall 0.308 val_loss 0.098 val acc 0.973 val recall 0.910 \n",
      "epoch58 -->  1861.8 examples/sec train_loss 0.353 train acc 0.934 train recall 0.310 val_loss 0.099 val acc 0.973 val recall 0.911 \n",
      "epoch59 -->  1750.4 examples/sec train_loss 0.351 train acc 0.938 train recall 0.306 val_loss 0.098 val acc 0.973 val recall 0.907 \n",
      "epoch60 -->  1672.7 examples/sec train_loss 0.359 train acc 0.929 train recall 0.301 val_loss 0.100 val acc 0.973 val recall 0.912 \n",
      "epoch61 -->  1857.0 examples/sec train_loss 0.351 train acc 0.928 train recall 0.304 val_loss 0.099 val acc 0.973 val recall 0.908 \n",
      "epoch62 -->  1807.2 examples/sec train_loss 0.345 train acc 0.933 train recall 0.313 val_loss 0.100 val acc 0.973 val recall 0.911 \n",
      "epoch63 -->  1899.4 examples/sec train_loss 0.347 train acc 0.932 train recall 0.307 val_loss 0.101 val acc 0.972 val recall 0.917 \n",
      "epoch64 -->  1782.9 examples/sec train_loss 0.348 train acc 0.930 train recall 0.306 val_loss 0.101 val acc 0.973 val recall 0.914 \n",
      "epoch65 -->  1854.1 examples/sec train_loss 0.342 train acc 0.934 train recall 0.308 val_loss 0.099 val acc 0.973 val recall 0.908 \n",
      "epoch66 -->  1870.6 examples/sec train_loss 0.330 train acc 0.934 train recall 0.325 val_loss 0.101 val acc 0.973 val recall 0.913 \n",
      "epoch67 -->  1876.5 examples/sec train_loss 0.344 train acc 0.937 train recall 0.312 val_loss 0.103 val acc 0.972 val recall 0.919 \n",
      "epoch68 -->  1856.0 examples/sec train_loss 0.353 train acc 0.935 train recall 0.311 val_loss 0.102 val acc 0.973 val recall 0.915 \n",
      "epoch69 -->  1810.9 examples/sec train_loss 0.348 train acc 0.936 train recall 0.310 val_loss 0.100 val acc 0.972 val recall 0.907 \n",
      "epoch70 -->  1745.4 examples/sec train_loss 0.354 train acc 0.928 train recall 0.303 val_loss 0.103 val acc 0.973 val recall 0.918 \n",
      "epoch71 -->  1854.1 examples/sec train_loss 0.347 train acc 0.934 train recall 0.308 val_loss 0.101 val acc 0.973 val recall 0.911 \n",
      "epoch72 -->  1846.4 examples/sec train_loss 0.351 train acc 0.936 train recall 0.307 val_loss 0.103 val acc 0.973 val recall 0.916 \n",
      "early stopping! best_val:0.945 epoch67\n"
     ]
    }
   ],
   "source": [
    "Net = TextCNN(embedding_dim, num_channels, kernel_sizes)\n",
    "trained_net = train_model(Net, train_iter, val_iter, num_epochs, lr, patience,\n",
    "                         savepath=savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"lstm.pth\"\n",
    "embedding_dim, hidden_size, num_layers = 100, 64, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 -->  787.0 examples/sec train_loss 0.408 train acc 0.859 train recall 0.134 val_loss 0.396 val acc 0.866 val recall 0.135 \n",
      "epoch2 -->  747.4 examples/sec train_loss 0.395 train acc 0.867 train recall 0.134 val_loss 0.394 val acc 0.866 val recall 0.135 \n",
      "epoch3 -->  577.6 examples/sec train_loss 0.396 train acc 0.866 train recall 0.135 val_loss 0.394 val acc 0.866 val recall 0.135 \n",
      "epoch4 -->  573.4 examples/sec train_loss 0.394 train acc 0.866 train recall 0.136 val_loss 0.403 val acc 0.866 val recall 0.135 \n",
      "epoch5 -->  579.9 examples/sec train_loss 0.390 train acc 0.866 train recall 0.143 val_loss 0.377 val acc 0.866 val recall 0.151 \n",
      "epoch6 -->  580.7 examples/sec train_loss 0.245 train acc 0.919 train recall 0.503 val_loss 0.234 val acc 0.923 val recall 0.561 \n",
      "epoch7 -->  585.3 examples/sec train_loss 0.180 train acc 0.942 train recall 0.622 val_loss 0.200 val acc 0.933 val recall 0.680 \n",
      "epoch8 -->  555.3 examples/sec train_loss 0.163 train acc 0.948 train recall 0.688 val_loss 0.277 val acc 0.919 val recall 0.775 \n",
      "epoch9 -->  561.1 examples/sec train_loss 0.167 train acc 0.949 train recall 0.667 val_loss 0.178 val acc 0.943 val recall 0.747 \n",
      "epoch10 -->  583.8 examples/sec train_loss 0.145 train acc 0.954 train recall 0.695 val_loss 0.159 val acc 0.950 val recall 0.722 \n",
      "epoch11 -->  576.0 examples/sec train_loss 0.142 train acc 0.955 train recall 0.728 val_loss 0.158 val acc 0.951 val recall 0.755 \n",
      "epoch12 -->  564.3 examples/sec train_loss 0.133 train acc 0.957 train recall 0.719 val_loss 0.162 val acc 0.947 val recall 0.778 \n",
      "epoch13 -->  553.1 examples/sec train_loss 0.132 train acc 0.958 train recall 0.737 val_loss 0.187 val acc 0.946 val recall 0.835 \n",
      "epoch14 -->  469.2 examples/sec train_loss 0.122 train acc 0.961 train recall 0.763 val_loss 0.152 val acc 0.947 val recall 0.664 \n",
      "epoch15 -->  443.1 examples/sec train_loss 0.126 train acc 0.961 train recall 0.727 val_loss 0.161 val acc 0.956 val recall 0.813 \n",
      "epoch16 -->  437.5 examples/sec train_loss 0.118 train acc 0.965 train recall 0.755 val_loss 0.145 val acc 0.960 val recall 0.791 \n",
      "epoch17 -->  435.4 examples/sec train_loss 0.124 train acc 0.961 train recall 0.754 val_loss 0.144 val acc 0.954 val recall 0.750 \n",
      "epoch18 -->  424.0 examples/sec train_loss 0.138 train acc 0.959 train recall 0.742 val_loss 0.142 val acc 0.955 val recall 0.762 \n",
      "early stopping! best_val:0.887 epoch13\n"
     ]
    }
   ],
   "source": [
    "Net = LSTM_FC(embedding_dim=embedding_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
    "trained_net = train_model(Net, train_iter, val_iter, num_epochs, lr, patience,\n",
    "                          savepath=savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# out-of-sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retsore the model(with best performance on validation set)\n",
    "\n",
    "trained_net = BaseModel()\n",
    "#trained_net = TextCNN(embedding_dim, num_channels, kernel_sizes)\n",
    "#trained_net = LSTM_FC(embedding_dim, hidden_size, num_layers)\n",
    "\n",
    "trained_net.load_state_dict(torch.load('FC.pth'))\n",
    "trained_net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9669642857142857, tensor(0.9183))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_acc, test_recall = evaluate(trained_net, test_iter, loss=nn.BCELoss(), acc_func=vanila_acc, threshold=0.5)\n",
    "test_acc, test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_predictions = trained_net(torch.tensor(X_test, dtype=torch.float32)).detach().numpy().ravel()\n",
    "test_predictions[test_predictions>=0.5] = 1.0\n",
    "test_predictions[test_predictions<0.5] = 0.0\n",
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       966\n",
      "           1       0.94      0.81      0.87       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.95      0.90      0.92      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapper for model based on pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchWrapper(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    def __init__(self, net, shape, path):\n",
    "        self.net = net\n",
    "        self.shape = shape # how to reshape X from 2D->3D\n",
    "        self.path = path  # path of pre-trained model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # detech different classes\n",
    "        self.classes_ = np.argsort(np.argsort(np.unique(y)))\n",
    "        # laod pre-trained model\n",
    "        self.net.load_state_dict(torch.load(self.path))\n",
    "        self.net.eval();\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)].ravel()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_ = X.reshape(X.shape[0], self.shape[0], self.shape[1])\n",
    "        probas_pos = self.net(torch.tensor(X_, dtype=torch.float32)).detach().numpy().ravel()\n",
    "        probas_neg = 1 - probas_pos\n",
    "        return np.c_[probas_neg, probas_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       966\n",
      "           1       0.96      0.87      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.97      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Net = TextCNN(embedding_dim, num_channels, kernel_sizes)\n",
    "textcnn = PytorchWrapper(Net, (35, 100), path=\"textcnn.pth\")\n",
    "textcnn.fit(X_train.reshape(X_train.shape[0], -1), Y_train.ravel());\n",
    "print(classification_report(Y_test, textcnn.predict(X_test.reshape(X_test.shape[0], -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       966\n",
      "           1       0.92      0.73      0.81       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.94      0.86      0.89      1115\n",
      "weighted avg       0.95      0.96      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Net = LSTM_FC(embedding_dim, hidden_size, num_layers)\n",
    "textcnn = PytorchWrapper(Net, (35, 100), path=\"lstm.pth\")\n",
    "textcnn.fit(X_train.reshape(X_train.shape[0], -1), Y_train.ravel());\n",
    "print(classification_report(Y_test, textcnn.predict(X_test.reshape(X_test.shape[0], -1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# truncate for quick validation\n",
    "X_train = X_train[:300, :]\n",
    "Y_train = Y_train[:300, :]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "    ('svr', SVC(kernel='rbf', random_state=42)),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)),\n",
    "    ('textcnn', PytorchWrapper(TextCNN(100,  [16,16,16,16], [2, 5, 10, 20]), (35, 100), \"textcnn.pth\")),\n",
    "    ('lstm', PytorchWrapper(LSTM_FC(100, 64, 2), (35, 100), \"lstm.pth\")),\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression(),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   35.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train.reshape(X_train.shape[0],-1), Y_train.ravel());\n",
    "clf_predictions = clf.predict(X_test.reshape(X_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       966\n",
      "           1       0.96      0.87      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.97      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, clf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48764465,  2.22427691,  0.37060004,  7.34373822, -0.40859267]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.final_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train.reshape(X_train.shape[0],-1), Y_train.ravel())\n",
    "rf_pred = rf.predict(X_test.reshape(X_test.shape[0],-1))\n",
    "\n",
    "svc = SVC(kernel='rbf', random_state=42)\n",
    "svc.fit(X_train.reshape(X_train.shape[0],-1), Y_train.ravel())\n",
    "svc_pred = svc.predict(X_test.reshape(X_test.shape[0],-1))\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "xgb.fit(X_train.reshape(X_train.shape[0],-1), Y_train.ravel())\n",
    "xgb_pred = xgb.predict(X_test.reshape(X_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net = TextCNN(embedding_dim, num_channels, kernel_sizes)\n",
    "trained_net.load_state_dict(torch.load('textcnn.pth'))\n",
    "trained_net.eval();\n",
    "with torch.no_grad():\n",
    "    textcnn_pred = trained_net(torch.tensor(X_test, dtype=torch.float32)).detach().numpy().ravel()\n",
    "textcnn_pred[textcnn_pred>=0.5] = 1.0\n",
    "textcnn_pred[textcnn_pred<0.5] = 0.0\n",
    "\n",
    "trained_net = LSTM_FC(embedding_dim, hidden_size, num_layers)\n",
    "trained_net.load_state_dict(torch.load('lstm.pth'))\n",
    "trained_net.eval();\n",
    "with torch.no_grad():\n",
    "    lstm_pred = trained_net(torch.tensor(X_test, dtype=torch.float32)).detach().numpy().ravel()\n",
    "lstm_pred[lstm_pred>=0.5] = 1.0\n",
    "lstm_pred[lstm_pred<0.5] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_corr(pred1, pred2):\n",
    "    print(f\"pos prediction for model1: {np.sum(pred1)/pred1.shape[0]:.4f}\")\n",
    "    print(f\"pos prediction for model2: {np.sum(pred2)/pred2.shape[0]:.4f}\")\n",
    "    print(f\"pos prediction for both model1 & model2: {np.sum(pred1+pred2 == 2)/pred1.shape[0]:.4f}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos prediction for model1: 0.0780\n",
      "pos prediction for model2: 0.1211\n",
      "pos prediction for both model1 & model2: 0.0780\n",
      "pos prediction for model1: 0.0834\n",
      "pos prediction for model2: 0.1211\n",
      "pos prediction for both model1 & model2: 0.0834\n",
      "pos prediction for model1: 0.0942\n",
      "pos prediction for model2: 0.1211\n",
      "pos prediction for both model1 & model2: 0.0915\n",
      "pos prediction for model1: 0.1067\n",
      "pos prediction for model2: 0.1211\n",
      "pos prediction for both model1 & model2: 0.1013\n"
     ]
    }
   ],
   "source": [
    "pred_corr(rf_pred, textcnn_pred)\n",
    "pred_corr(svc_pred, textcnn_pred)\n",
    "pred_corr(xgb_pred, textcnn_pred)\n",
    "pred_corr(lstm_pred, textcnn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
